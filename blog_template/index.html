<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>Zigzagging: A Comparison Between Latency and Throughput in KVPR</title>
      <meta property="og:title" content="Zigzagging: A Comparison Between Latency and Throughput in KVPR" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">Zigzagging: A Comparison Between Latency and Throughput in KVPR</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px">Andrea K. Leang</span>
										</td>
										<td align=left>
												<span style="font-size:17px">Visal Soeun</span>
										</td>
										<td align=left>
												<span style="font-size:17px">Kevin Min</span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
				</div>
				<div class="margin-right-block" style="transform: translate(0%, -100%);"></div>
				
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#kv_cache_offloading_strategies">KV Cache Offloading Strategies</a><br><br>
			  <a href="#Experiment_Method">Experimental Method</a><br><br>
			  <a href="#Experiment_Results">Experiment Results</a><br><br>
              <a href="#Discussion">Discussion</a><br><br>
			  <a href="#Limitations">Limitations</a><br><br>
			  <a href="#Future Work">Future Work</a><br><br>
			  <a href="#citations">References</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
		    </div>
			<div class="margin-right-block" style="transform: translate(0%, -100%);"></div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<h1>Introduction</h1>
				Large language models (LLMs) are deep learning models that often utilize self-attention, where generating a single token requires attending to all previously 
				generated tokens <a href="#Viswani">[1]</a>. This generally requires quadratic computation complexity due to 
				having to revisit previous tokens. As the number of tokens generated increases, the memory and time required 
				to generate the next token becomes a significant problem in the training and testing process of LLMs.
				<br><br>
				The demands on LLMs have grown non-stop: faster results, longer answers, and harder problems. To accommodate the appetite of the masses, current models and 
				the contexts they are given have been growing exponentially in size. The increased model sizes allow for more minute details to be finetuned. The longer contexts 
				come from the increased amount of information needed to solve the harder problems and the long back and forth chats, where the model is expected to remember every 
				detail of what has been said so far. The amount of shared memory between prompts also increases to help the models speed up computation, allowing them to transfer 
				less information during training.
				<br><br>
				With their immense growing sizes, the model, its context/prompts, and the subsequently generated values can no longer all fit on a single GPU. Real-world models 
				are now shifting towards a “heterogeneous” solution, where part of the data is saved on the CPU and moved to the GPU as needed. 
				<br><br>
				One crucial case is KV cache: intermediate computation results during LLM inference saved or “cached” to avoid recomputation in the popular transformer models. 
				KV cache scales linearly with token generation and currently dominates the memory footprint for long-context inference, exceeding the current GPU memory capacity. 
				<br><br>
				However, due to the sheer amount of KV cache generated during inference, the offloading of KV cache onto the CPU becomes a new bottleneck in power and performance: 
				what originally was data movement within the GPU now requires extra time and power in moving immense amounts of KV cache back to the CPU to continue token generation. 
				Because token generation occurs in the GPU, the model weights must be stored in the GPU during each training cycle, leaving only a certain amount of space available 
				for the KV cache on the GPU. But the increasing size of the models mean that model weights take up more space while KV caches also require more and more GPU space 
				for increased efficiency. As such, there is a bottleneck for the available space in the GPU.
				<br><br>
				To face this great bottleneck, researchers have developed different KV cache offloading strategies to hide the induced latency. 


			</div>
			<div class="margin-right-block" style="transform: translate(0%, -100%);"></div>
		</div>

		<div class="content-margin-container" id="kv_cache_offloading_strategies">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>KV Cache Offloading Strategies</h1>

					A common KV cache offloading strategy to emerge is scheduling when KV cache is 
					loaded from the CPU by predicting which KV cache will be used next and prefetching them. 
					This reduces the time that the GPU is stalled, waiting for the KV cache, but this prediction may result in “cache misses” 
					when the predictor mispredicts where the needed KV cache block is, i.e. if the KV cache isn’t on the GPU because it was not prefetched 
					fast enough <a href="#CLO">[2]</a>, or the wrong KV cache block prefetched <a href="#Async">[3]</a>. This misprediction introduces new overhead in either refetching or 
					recomputing the needed KV cache. While these strategies introduce an ideal cache hit rate, different workloads and batching may result in 
					different experienced cache hit rates, making it hard to reliably calculate latency and power <a href="#CLO">[2,3]</a>.
					<br><br>
					Other scheduling strategies look at a bigger picture: changing the order that layers will be processed in batches. Two prominent schedules are row-by-row, aiming to minimize 
					latency by trying to get the first tokens out as soon as possible, and column-by-column, maximizing throughput by loading multiple KV 
					caches at once to process multiple sequences in parallel, as shown in Figure 1.
					<br><br>
					<center>
						<img src="./images/rbr_and_cbc.png" width=256px/>
						Figure 1: Row-by-row and column-by-column schedules <a href="#KVPR">[4]</a>
					</center>

					<br><br>
					A further improvement on KV caching involves loading part of the KV cache onto the GPU, while simultaneously recomputing the rest of the cache. This has the benefit of avoiding GPU idle time, 
					when the CPU is loading the KV cache onto the GPU, as the GPU can now by computing values during this timeframe and thus reducing the effective latency. This approach is described in [KVPR, Both].
					 In this blog, we will expand on the approach taken in the KVPR paper, which has a schedule-independent improvement; with this predictable KV cache offloading strategy as a basis, we can explore the
					 influence of different KV cache offloading schedules on the final performance and the relationship between the throughput and latency for different schedules. 

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		    </div>
		</div>

		<div class="content-margin-container" id="Experiment_Method">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
			<h1>Experimental Method</h1>
			We chose to design our experiment to fairly compare different scheduling strategies, which led us to take inspiration from the 
			“zig-zag” scheduling described in ZvLLM <a href="#ZvLLM">[5]</a>. ZvLLM expands upon the two scheduling strategies mentioned in KVPR 
			(namely, row-by-row and column-by-column) by generalizing the full column-by-column schedule as shown in Figure 2.
			<br><br>
			<center>
				<img src="./images/zig_zag_schedule.png" width=256px/>
				Figure 2: Zig-zag scheduling <a href="#ZvLLM">[5]</a>
			</center>
			<br><br>
			As shown in Figure 2, the zig-zag schedule processes batches at a time. For each batch, it looks at the first layer of all the 
			sequences in that batch, loads the relevant caches and weights onto the GPU in parallel, then moves on to the next layer of this batch. 
			Upon finishing with these batches it then proceeds to the next batch. This method improves the throughput of the scheduling, as multiple 
			sequences are processed in parallel; however, the tradeoff is that the latency is worse. 
			<br><br>
			Under this interpretation, the row-by-row scheduling in the KVPR paper can be interpreted as an extreme version of the zig-zag scheduling, 
			with a batch size of one. The column-by-column scheduling is the other end of the extreme, with a single batch of maximal possible size 
			equal to the number of sequences processed. 
			<br><br>	
			However, there is no mention of the specific experiment values of this zig-zag strategy, such as block size, total number of batches, etc. 
			Our experiment fills in this gap, and explores in more depth how the latency and throughput change as we vary different parameters. This 
			analysis allows us to look at the tradeoff between latency and throughput, so that potential users can adapt their scheduling process based 
			on how important these two factors are to them.
			<br><br>
			Since KVPR is independent of scheduling, we utilized it as the base offloading strategy for each zigzag block. This foundation ensures that 
			the changes in latency and throughput are caused purely by the changes in scheduling. 
			<br><br>
			During our initial experiments, we kept the total number of prompts, prompt length, generation length, and other parameters of KVPR fixed, 
			effectively only changing the number of blocks and block size with each test. We run these experiments over several different values for 
			the number of sequences (8, 16, 32, 64) and record the latency and throughput. In the next section, we then look at the results when varying other 
			parameters like prompt and generation length, to see if any initial trends we observed stay consistent. 
			<br><br>
			All experiments were run on Google Collab Pro using an A100 GPU, on the high RAM option, which typically has around 80-90 GB, 
			far exceeding the amounts we needed for our experiments.
			<br><br>

		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>

	<div class="content-margin-container" id="Experiment_Results">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
			<h1>Experiment Results</h1>
			Through the design of our experiment, our baseline values for KVPR’s pure column-by-column and row-by-row schedules are the integrated 
			in trials where the GPU batch size is 1 (every batch is included in the block) and where the CPU batch size is the number of prompts 
			(every prompt is in its own batch).
			<br><br>
			As we can see from Figure 3, the latency for one sequence goes up as the batch size increases, and conversely the throughput goes up. 
			This matches with the expectation from KVPR <a href="#KVPR">[4]</a>, where the extreme cases of column-by-column scheduling and row-by-row scheduling 
			yielded the highest throughput and lowest latency respectively. 

			<center>
				<img src="./images/combined_3_bars.png" />
				Figure 3: Latency and Throughput vs. Batch Size for Different Number of Total Prompts 
			</center>

			<br><br>
			However, the additional inclusion of results for intermediate batch sizes allows us to analyze a clearer relationship between latency 
			and throughput with our zigzag blocks. In particular, we observe a sub-logarithmic growth for throughput as a 
			function of batch size, while latency per sequence appears to have a roughly linear growth (note the exponential growth for 
			batch sizes along this x-axis, so the linear growth looks exponential in the figure). This latter effect results from the fact that 
			total latency tends to follow a roughly constant, slightly decaying trend with respect to batch size. This is because batching in larger 
			sections allows us to load more KV caches at once, which effectively saves overall time. However, the tradeoff is that each individual 
			sequence is significantly slower (and roughly inversely proportional to the batch size), since only a decreasing proportion of the total 
			GPU resources can be devoted to processing that particular sequence.
			<br><br>
			Additionally, as we look at different values for the number of sequences considered, the change in total latency as batch size increases 
			tends to become more extreme when the number of sequences goes up. This is because we only need to load model weights onto the GPU once 
			per batch, so the number of times we do this load is proportional to the batch size. For example, as we go from batch size one (i.e., 
			row-by-row scheduling) to batch size two, the number of model weight loadings goes from N to N / 2, where N is the total number of 
			sequences; so the improvement in total latency resulting from this improvement is proportional to N. Because the model weights are 
			relatively large in memory, the effect this has on the overall latency is large, and this trend is reflected in Figure 4, 
			where we can observe a roughly linear relationship between N and the improvement in latency when going from batch size of one to batch size of two.
			<br><br>
			<center>
				<img src="./images/latency_improvement.jpg" width="512px"/>
				Figure 4: Latency Improvement vs. Total Number of Prompts 
			</center>
			<br><br>
			Furthermore, we can see in Figure 6 that the general trends of latency and throughput versus batch size does not depend on generation length 
			or scheduling length, so these results are applicable to a variety of different use cases. Any small variances can likely be attributed to 
			physical hardware limitations and differences. We can also observe some other trends in this figure. The throughput tends to go down when 
			prompt length does, but it is mostly independent of generation length. This is because throughput heavily depends on prompt length, since 
			the prompt length is significantly larger than generation length and as such it is the primary factor in KV cache size, which affects the 
			throughput here. Additionally, because throughput is tokens generated per second, increasing prompt length does not increase tokens generated 
			while it does increase time, so throughput gets higher. In contrast, increasing generation length does not affect KV cache size by as much, 
			and also increases the tokens generated in proportion to the time, so throughput is mostly constant.
			<br><br>
			<center>
				<img src="./images/lat_thr_combined.png" width=1024px/>
				Figure 5: Latency and Throughput vs. GPU Batch Size
			</center>
			<br><br>
			If we look at the total latency, we can see it tends to go up as either prompt length or generation length does. It’s fairly linear with 
			respect to the generated length given a fixed prompt length, as we might expect, since the number of multiplications per sequence is a linear 
			function of the generated length. But if we fix the generated length and let the prompt length vary, we note that although the latency increases 
			when prompt length does, this relationship is not linear. This is likely due to the fact that the number of layers is unchanged, so there is no 
			increase in latency there, but the significant increase in KV cache size caused by the longer prompt leads each layer to be slower by a sub-linear factor. 
			<br><br>
			Prior to using Google Colab to run our experiments, we tried to use MIT’s Engaging-On-Demand cluster, which also provides custom GPUs on which we can 
			run our models. Figure 7 shows the results from using Engaging’s L40 GPUs. We observe similar trends to what we found on Colab: the latency per sequence 
			increases linearly with GPU batch size while throughput goes through a sub-logarithmic growth. However, we noticed there are inconsistencies in our 
			results with Engaging. Between 16 and 32 sequences, we noticed that the average total latency actually went down, which does not make sense since more 
			sequences would require more time to attend to and thus more latency. We also noticed that the general trend did not hold for 64 sequences, where the 
			total throughput for batch sizes 32 and 64 were lower than the throughput for batch size 16, and the total latency was also higher.
			<br><br>
			<center>
				<img src="./images/combined_3_bars_andrea.png" width=1024px/>
				Figure 6: Engaging Results: Latency and Throughput vs GPU Batch Size
			</center>
			<br><br>
			Upon investigation, Engaging is a service that allocates jobs to the resources it has, which can result in multiple jobs being assigned to the same GPU. 
			This causes contention in both memory and GPU computation resources. The trials out of the trend, mentioned before, were indeed found to be run with other 
			processes on the same GPU. Because we did not have access to the whole GPU, there were differences between latency and throughput values proportional to 
			how much of a GPU our jobs were assigned, even for trials with the same number of sequences. The smaller variances in these values recorded against the 
			Colab results could also be due to the different GPUs’ computation capabilities, with the A100 being able to run processes faster in general.
			Additionally, as we look at different values for the number of sequences considered, the change in total latency as batch size increases 
			tends to become more extreme when the number of sequences goes up. This is because we only need to load model weights onto the GPU once 
			per batch, so the number of times we do this load is proportional to the batch size. For example, as we go from batch size one (i.e., 
			row-by-row scheduling) to batch size two, the number of model weight loadings goes from N to N / 2, where N is the total number of 
			sequences; so the improvement in total latency resulting from this improvement is proportional to N. Because the model weights are 
			relatively large in memory, the effect this has on the overall latency is large, and this trend is reflected in Figure 4, 
			where we can observe a roughly linear relationship between N and the improvement in latency when going from batch size of one to batch size of two.
			

		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>

	
	

	<div class="content-margin-container" id="Discussion">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
			<h1>Discussion</h1>
			Our results are significant because they show that there exists a scheduler which uses a specific number of zigzags which optimizes for the 
			highest increase in marginal throughput for the lowest increase in marginal latency. This also means that we can customize the scheduler if 
			a higher throughput is preferred over lower latency or lower latency over higher throughput. 
			<br><br>
			The observed relationship between latency and throughput also confirms that, for the schedules examined, the row-by-row schedule optimizes 
			total latency while the column-by-column schedule optimizes both latency per sequence and total throughput. This confirms that the two extreme 
			schedules are optimized for different metrics for all workloads. 
			<br><br>
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>
	<div class="content-margin-container" id="Limitations">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
			<h1>Limitations</h1>
			While our analysis provides a deeper investigation into the tradeoffs between latency and throughput, we are limited by the size and complexity 
			of the models that we can use. Due to limitations in memory and compute available to us, we could not gather reliable data for sequences larger than 64. 
			In state-of-the-art models, the number of sequences used in training are generally as large as possible to reduce variabilities and speed up training, 
			but we are unable to investigate at that scale.
			<br><br>
			Because our prompt size and generation size are both relatively small, being only on the magnitudes of hundreds to thousands of tokens, the size of our 
			KV cache is not extremely large, meaning we don’t face as many issues due to memory. In real-world applications, GPU memory space is extremely precious, 
			and solutions must be implemented to efficiently utilize this memory. Since memory is not such an important bottleneck in our model, we have the liberty 
			to only implement scheduling methods without needing to implement other forms of KV cache offloading. This, of course, doesn’t necessarily reflect state-of-the-art 
			training paradigm, where scheduling would have to be adjusted around other more necessary methods to reduce the strain on the GPU. 
			<br><br>
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>
	<div class="content-margin-container" id="Future Work">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
			<h1>Future Work</h1>
			While we only looked at the latency and throughput metrics, there are also other metrics that we can aim to optimize in order to speed up training times. 
			For instance, Total Kernel Launch and Queuing Time (TKLQT) describes the total time that is wasted in the communication between the CPU and GPU, lost either 
			due to kernel launching time in the CPU as the stored kernels are sent to the GPU, or due to queuing in the GPU where the GPU running capacity cannot keep 
			up with CPU commands <a href="#TKLQT">[6]</a>. TKLQT indicates the PU-boundness of a system and workload, letting the user know if most of the stalling is 
			due to the CPU or GPU operation durations.
			<br><br>
			We also only explored schedules that contained fixed-sized blocks of column-by-column subschedules. Future work can explore schedules that vary the block 
			sizes within a schedule. These schedules will explore a new dimension of timing: when in the inference would big zigzag blocks be useful? Patterns of zigzag 
			blocks may reveal a hidden optimal schedule for latency or throughput. 
			<br><br>
			We can also further investigate the effects of zigzag on asymmetric GPU-CPU pipelining, an offloading strategy that involves splitting each batch into sub-batches, 
			computing a sub-batch’s decoding of attention and KV caching in the CPU, and the other sub-batch in the GPU <a href="#NEO">[7]</a>. This presents an interesting prospect, 
			where we may be able to investigate the magnitude to which scheduling affects the latency and throughput in GPU as compared to the CPU. We would expect an insignificant 
			result in the CPU due to computations being in series, compared to the significant improvements in the parallelized GPUs, but it would nonetheless be a question to investigate.
			<br><br>
			We’ve also noted that the KV cache size reaches fairly large numbers as we increase the prompt and generation lengths, reaching around 20 GB, and if we continue increasing 
			these parameters it could cause the CPU memory to run out for large enough workloads. An area of KV cache offloading strategies is dedicated towards studying exactly 
			these extreme cases, like multi-turn dialogues with chatbots, where there isn’t enough storage even on the CPU DRAM. These strategies explore storing KV cache in multiple 
			layers of the CPU: DRAM, SSD, and Disk <a href="FlashGen">[8]</a>. With new levels, there are more opportunities to experiment with different combinations of schedules. With speculation on 
			how often KV cache is loaded back from the different levels, we expect schedules with lower latencies to be prioritized near the DRAM and schedules that optimize throughput to be closer to the disk.
			<br><br>
			Another major area of KV cache offloading strategies that we have not explored is reducing the size of KV cache saved and moved used in token generation through sacrificing the quality of the output. 
			The 3 main strategies in current literature to reduce KV cache size are compression, quantization, and pruning various sizes and parts of KV cache <a href="Rocket">[9]</a><a href="A2ATS">[10]</a>. In combination with exploring the different patterned 
			schedules, pairing different KV cache reduction strategies may reduce the latency and increase throughput in all types of scheduling, and investigating the relation between those two approaches could 
			be another area to look into.
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		</div>
	</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="Viswani"></a>[1] <a href="https://arxiv.org/pdf/1706.03762">Attention Is All You Need</a>, Viswani et al., 2017<br><br>
							<a id="CLO"></a>[2] <a href="https://arxiv.org/abs/2504.06319">CLO: Efficient LLM Inference System with CPU-Light
								KVCache Offloading via Algorithm-System Co-Design</a>, Yi et al., 2025<br><br>
							<a id="ASnyc"></a>[3] <a href="https://arxiv.org/pdf/2504.06319">Accelerating LLM Inference Throughput
								via Asynchronous KV Cache Prefetching</a>, Dong et al., 2025<br><br>
							<a id="KVPR"></a>[4] <a href="https://arxiv.org/pdf/2411.17089">KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial Recomputation</a>, Jiang Chaoyi et al., 2025<br><br>
							<a id="ZvLLM"></a>[5] <a href="https://adiprerepa.github.io/data/598final.pdf">ZvLLM: Zigzag forward pass with vLLM</a>, Zhu et al., 2025<br><br>
							<a id="TKLQT"></a>[6] <a href="https://arxiv.org/pdf/2504.11750">Characterizing and Optimizing LLM Inference
								Workloads on CPU-GPU Coupled Architectures</a>, Vellaisamy et al., 2025<br><br>
							<a id="NEO"></a>[7] <a href="https://yangzhou1997.github.io/paper/neo_mlsys25.pdf?utNEO:%20SAVING%20GPU%20MEMORY%20CRISIS%20WITH%20CPU%20OFFLOADING%20FOR%20ONLINE%20LLM%20INFERENCE">NEO: Saving GPU Memory Crisis With CPU Offloading For Online LLM Inference</a>, Jiang Xuanlin et al., 2025<br><br>
							<a id="FlashGen"></a>[8] <a href="https://dl.acm.org/doi/pdf/10.1145/3676641.3716245">Accelerating LLM Serving for Multi-turn Dialogues with Efficient Resource Management</a>, Jinwoo Jeong and Jeongseob Ahn, 2025<br><br>
							<a id="Rocket"></a>[9] <a href="https://arxiv.org/html/2502.14051v2">RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression</a>, Behnam et al., 2025<br><br>
							<a id="A2ATS"></a>[10] <a href="https://arxiv.org/abs/2502.12665">A<sup>2</sup>ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization</a>, He et al., 2025<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>
